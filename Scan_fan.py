# ********************___04_SelRu_00__draft___************************
'''
–ó–∞–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç Rutracker —á–µ—Ä–µ–∑ —Ö—Ä–æ–º —Å vpn –∏ –∏—Å–∫–∞—Ç—å —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏ –≤—ã–≥–æ–¥–Ω—ã–µ —Ç–æ—Ä—Ä–µ–Ω—Ç—ã
Legend:
üü¢  0)  –ë—ã—Ç—å —á–µ—Ç–∫–∏–º –ø–æ—Ü–∞–Ω—á–∏–∫–æ–º
üü°  0)  –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–º
üî¥  0)  –°–¥–µ–ª–∞—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–µ

üî¥  1)  –ó–∞–º–µ–Ω–∏—Ç—å Readme
üü¢  2)  –ó–∞–º–µ–Ω–∏—Ç—å –°–∞–π—Ç (—Å—Ä–∞–∑—É —Å –ø–æ–∏—Å–∫–æ–º 2021 –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –ø–æ –ª–∏—á–∞–º)
üî¥  3)  –í—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–µ –≤—ã–≥–æ–¥–Ω—ã–µ —Ç–æ—Ä—Ä–µ–Ω—Ç—ã
'''
# ***********************************************************************

# import requests
# from fake_useragent import UserAgent
# from bs4 import BeautifulSoup
# from selenium.webdriver import Chrome
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
'''
caps = DesiredCapabilities().CHROME
# caps["pageLoadStrategy"] = "normal"  #  complete
caps["pageLoadStrategy"] = "eager"  #  interactive (FAST GET)
#caps["pageLoadStrategy"] = "none"
driver = webdriver.Chrome(executable_path=catalog, chrome_options=chrome_options, desired_capabilities=caps)
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
'''

from selenium import webdriver
from selenium.webdriver.support.ui import Select
import pandas as pd
import time
import config

chrome_options = webdriver.ChromeOptions()

# For linux
chrome_options.add_argument('user-data-dir=/home/'+ config.user_name +'/.config/google-chrome')
catalog = '/home/'+ config.user_name +'/.local/bin/chromedriver' #–ö–∞—Ç–∞–ª–æ–≥ –∫—É–¥–∞ —Å–∫–∞—á–∞–ª–∏ webdriver (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ PATH)

# For Windows
# chrome_options.add_argument('user-data-dir=C:\\Users\\'+ config.user_name +'\\AppData\\Local\\Google\\Chrome\\User Data')
# catalog = "C:\\Users\\"+ config.user_name +"\\AppData\\Local\\Temp\\chromedriver.exe" #–ö–∞—Ç–∞–ª–æ–≥ –∫—É–¥–∞ —Å–∫–∞—á–∞–ª–∏ webdriver (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ PATH)

chrome_options.add_argument("--incognito")
chrome_options.add_argument
driver = webdriver.Chrome(executable_path=catalog, chrome_options=chrome_options)

# url = 'https://quotes.toscrape.com/'
# url = 'https://2ip.ru/'
# url = 'https://www.pirateproxy-bay.com'
# url = 'https://rutracker.org/forum/tracker.php?nm=2021'
url = 'https://rutracker.org'

# –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
driver.get(url)
# time.sleep(10)
driver.find_element_by_link_text("–í—Ö–æ–¥").click()
driver.find_element_by_name(name="login_username").send_keys(config.login)
driver.find_element_by_name(name="login_password").send_keys(config.password)
driver.find_element_by_name(name="login").click()
driver.find_element_by_id(id_="search-text").send_keys(2021)
driver.find_element_by_id(id_="search-submit").click()
select=Select(driver.find_element_by_name(name="o")) #sort by lich
select.select_by_value("11")
driver.find_element_by_id(id_="tr-submit-btn").click()

len_pages = 10
len_lines = 50

i=1
while i<=10:
    for x in range (1, len_lines+1):

        #find value
        pretty=False
        try:
            col_value = driver.find_element_by_xpath("//tbody/tr["+str(x)+"]/td[6]/a")
            col_sid = driver.find_element_by_xpath("//tbody/tr["+str(x)+"]/td[7]/b")
            col_lich = driver.find_element_by_xpath("//tbody/tr["+str(x)+"]/td[8]")
        except:
            break

        sid=float(col_sid.text)
        lich=float(col_lich.text)
        dim_value=col_value.text[-4]
        value=float(col_value.text[0:-5])

        if sid>0 and lich>0 and lich/sid>config.minLS:
            if dim_value=="M":
                pretty = True
            elif dim_value=="G" and value<config.maxGB:
                pretty = True
            else:
                pretty = False

        if pretty==True:
            driver.find_element_by_xpath("//tbody/tr["+str(x)+"]/td[4]/div/a").click()
            driver.find_element_by_link_text("–°–∫–∞—á–∞—Ç—å –ø–æ magnet-—Å—Å—ã–ª–∫–µ").click()
            # time.sleep(10)
            driver.find_element_by_id(id_="thx-btn").click()                            #To say thanks
            driver.back()
        print(x)
    if i!=10:
        driver.find_element_by_link_text("–°–ª–µ–¥.").click()
    print("***Page_"+str(i)+"_close")
    i+=1
driver.close()



# –ú—É—Å–æ—Ä–∫–∞ 4
# '16.49 GB ‚Üì'
# <input id="tr-submit-btn" class="bold" type="submit" value="–ü–æ–∏—Å–∫" style="width: 140px;">
# text_top100 = driver.find_element_by_name(title="Top 100")
# driver.find_element_by_link_text("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—á–µ–π").click()
# time.sleep(5)
'''
# driver.find_element_by_xpath("//a[@href='/top/48hall']").click()
len_lines = 100
# len_lines = 3
lines = []
lines_stop = []
for x in range (1, len_lines+1):

    # line = driver.find_element_by_xpath("//tr")
    # line = driver.find_element_by_xpath("//tbody/tr["+str(x)+"]")
    column1 = driver.find_element_by_xpath("//tbody/tr["+str(x)+"]/td[1]")
    indicator=column1.text[0]+column1.text[1]+column1.text[2]+column1.text[3]
    # print(indicator)
    if indicator!='Porn':
        try:
            driver.find_element_by_xpath("//tbody/tr["+str(x)+"]/td[2]").click()
            driver.find_element_by_xpath("//a[@title='Get this torrent']").click()
            # driver.find_element_by_link_text("MAGNET").click()
            driver.back()
            # lines.append(line)
        except:
            driver.get(url)
            driver.find_element_by_link_text("Top 100").click()
            driver.find_element_by_xpath("//a[@href='/top/48hall']").click()
    else:
        # lines_stop.append(line)
        pass
# print(len(lines))
# print(len(lines_stop))
'''

'''–ú—É—Å–æ—Ä–∫–∞ 3
# Error get this torrent
<a style="background-image: url('/static/img/icons/icon-magnet.gif');" href="magnet:?xt=urn:btih:DA51F364517AEF3934664A5E8C869C30A4BC0BE4&amp;dn=Kenan.S01E01.HDTV.x264-PHOENiX%5BTGx%5D&amp;tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&amp;tr=udp%3A%2F%2F9.rarbg.to%3A2920%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&amp;tr=udp%3A%2F%2Ftracker.internetwarriors.net%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.pirateparty.gr%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.cyberia.is%3A6969%2Fannounce" title="Get this torrent" target="_blank">&nbsp;Magnet</a>
'''

# –ú—É—Å–æ—Ä–∫–∞ 2
# time.sleep(1)
# chrome_options.add_argument('headless')

'''
–ú—É—Å–æ—Ä–∫–∞ 1
pages = 10
for page in range(1, pages):
    url = "http://quotes.toscrape.com/js/page/" + str(page) + "/"
    driver.get(url)
    items = len(driver.find_elements_by_class_name("quote"))
    total = []
    for item in range(items):
        quotes = driver.find_elements_by_class_name("quote")
        for quote in quotes:
            quote_text = quote.find_element_by_class_name('text').text
            author = quote.find_element_by_class_name('author').text
            new = ((quote_text, author))
            total.append(new)
    df = pd.DataFrame(total, columns=['quote', 'author'])
    df.to_csv('quoted.csv')
driver.close()
'''

'''
–ú—É—Å–æ—Ä–∫–∞ 0
response = driver.get(url)
# response = requests.get(url, headers={'User-Agent': UserAgent().chrome})
# response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

# print(soup)
file=open(("test.html"),"w")
file.write(str(soup))
file.close()
'''

'''
<a href="https://www1.thepiratebay3.to/top" title="Top 100" class="clickopen_" data-open="https://www.get-express-vpn.com/offer/torrent-vpn-2?a_fid=proxyspace&amp;offer=3monthsfree" rel="nofollow">Top 100</a>
'''

'''
<a title="Top torrents uploaded in the last 48 hours" href="/top/48hall">48h</a>
'''
